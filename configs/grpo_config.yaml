# GRPO (Group Relative Policy Optimization) Configuration
# Based on Tunix GRPO implementation

model:
  name: "gemma2-2b"  # or "gemma3-1b"
  max_length: 2048
  temperature: 0.7
  top_p: 0.9
  top_k: 50

training:
  method: "grpo"  # Group Relative Policy Optimization
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1e-5
  warmup_steps: 100
  max_steps: 2000
  save_steps: 500
  eval_steps: 250
  logging_steps: 50
  
  # GRPO specific parameters
  group_size: 4  # Number of responses per prompt for group comparison
  kl_coefficient: 0.1  # KL divergence penalty coefficient
  clip_range: 0.2  # PPO clip range

reward:
  type: "rubric_based"
  config_file: "configs/reward_config.yaml"
  
  # Reward composition
  reasoning_weight: 0.4
  answer_weight: 0.3
  coherence_weight: 0.2
  clarity_weight: 0.1

data:
  train_file: "data/synthetic_reasoning_set.jsonl"
  eval_file: "data/synthetic_reasoning_set.jsonl"  # Can be separate eval set
  max_samples: null  # null = use all data
  
output:
  format: "reasoning_trace"
  reasoning_tag: "<reasoning>"
  answer_tag: "<answer>"
  
hardware:
  device: "cpu"  # "cpu", "gpu", or "tpu"
  mixed_precision: false  # Set to true for GPU/TPU
  compile: true  # JAX compilation for speed

